# å¯¼å…¥æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
import os  # ç”¨äºæ“ä½œç³»ç»ŸåŠŸèƒ½ï¼Œå¦‚æ–‡ä»¶å’Œè·¯å¾„ç®¡ç†
from functools import partial  # ç”¨äºéƒ¨åˆ†åº”ç”¨å‡½æ•°

# å¯¼å…¥Numpyå’ŒPyTorchç›¸å…³åº“
import numpy as np  # ç”¨äºæ•°å€¼è®¡ç®—
import torch  # å¯¼å…¥PyTorchåº“
import torch.optim as optim  # å¯¼å…¥PyTorchçš„ä¼˜åŒ–å™¨æ¨¡å—
from torch.utils.data import DataLoader  # å¯¼å…¥æ•°æ®åŠ è½½å™¨æ¨¡å—
import time
import datetime
import subprocess

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å’Œæ¨¡å‹
# æ”¯æŒå¢å¼ºç‰ˆå’ŒåŸç‰ˆUNet
try:
    from model.unet_resnet_enhanced import Unet  # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆ
except ImportError:
    from model.unet_resnet import Unet  # å¦‚æœå¢å¼ºç‰ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸç‰ˆ

from model.unet_training import get_lr_scheduler, set_optimizer_lr, weights_init  # å¯¼å…¥ä¸U-Netè®­ç»ƒç›¸å…³çš„å‡½æ•°ï¼ˆå¦‚å­¦ä¹ ç‡è°ƒåº¦ã€è®¾ç½®ä¼˜åŒ–å™¨å­¦ä¹ ç‡å’Œæƒé‡åˆå§‹åŒ–ï¼‰
from utils.dataloader import UnetDataset, unet_dataset_collate  # å¯¼å…¥U-Netæ•°æ®é›†åŠå…¶åˆå¹¶å‡½æ•°
from utils.utils import seed_everything, worker_init_fn  # å¯¼å…¥ä¸€äº›å·¥å…·å‡½æ•°ï¼ˆå¦‚éšæœºç§å­è®¾ç½®ã€å±•ç¤ºé…ç½®å’Œåˆå§‹åŒ–å·¥ä½œçº¿ç¨‹ï¼‰
from utils.train_and_eval import train_one_epoch, evaluate  # å¯¼å…¥è®­ç»ƒä¸€ä¸ªepochçš„å‡½æ•°

from utils.create_exp_folder import create_exp_folder  # ç”¨äºåˆ›å»ºå®éªŒç›®å½•
from utils.plot_results import plot_training_curves  # ç»˜åˆ¶æ¨¡å‹ç»“æœå›¾

# GPUå ç”¨è®¡ç®—å‡½æ•°
def get_gpu_usage():
    result = subprocess.check_output(
        ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'],
        encoding='utf-8'
    )
    # è¾“å‡ºç±»ä¼¼ï¼š '1203, 6144\n'
    used, total = map(int, result.strip().split(','))

    return used
    # print(f"GPU æ˜¾å­˜å ç”¨: {used} MB / {total} MB")


def create_model(num_classes, weights, use_enhanced=True, use_attention=True, use_transpose=True, use_nucleus_cytoplasm=True):
    """
    åˆ›å»ºUNetæ¨¡å‹
    
    Args:
        num_classes: åˆ†ç±»æ•°é‡ï¼ˆè¾“å‡ºé€šé“æ•°ï¼‰
        weights: é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        use_enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆï¼ˆé»˜è®¤Trueï¼‰
        use_attention: æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›é—¨æ§ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰
        use_transpose: æ˜¯å¦ä½¿ç”¨è½¬ç½®å·ç§¯ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰
        use_nucleus_cytoplasm: æ˜¯å¦ä½¿ç”¨æ ¸è´¨åˆ†ç¦»æ„ŸçŸ¥æ¨¡å—ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰
    """
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    if use_enhanced:
        # ä½¿ç”¨å¢å¼ºç‰ˆUNet
        model = Unet(num_classes=num_classes, enhanced=True, 
                    use_attention=use_attention, use_transpose=use_transpose,
                    use_nucleus_cytoplasm=use_nucleus_cytoplasm)
        print(f"âœ… ä½¿ç”¨å¢å¼ºç‰ˆUNet (æ³¨æ„åŠ›é—¨æ§={use_attention}, è½¬ç½®å·ç§¯={use_transpose}, æ ¸è´¨åˆ†ç¦»={use_nucleus_cytoplasm})")
    else:
        # ä½¿ç”¨åŸç‰ˆUNet
        model = Unet(num_classes=num_classes, enhanced=False)
        print("âœ… ä½¿ç”¨åŸç‰ˆUNet")
    
    # ä½¿ç”¨ weights_init å‡½æ•°åˆå§‹åŒ–æ¨¡å‹æƒé‡
    weights_init(model)

    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œå³æ˜¯å¦éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡
    if weights:
        # è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼ˆå³æ¨¡å‹çš„å‚æ•°ï¼‰
        model_dict = model.state_dict()

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡å­—å…¸
        pretrained_dict = torch.load(weights, map_location='cpu')

        # åˆå§‹åŒ–ä¸‰ä¸ªåˆ—è¡¨å’Œå­—å…¸ï¼Œç”¨äºå­˜å‚¨åŠ è½½å’ŒæœªåŠ è½½çš„å‚æ•°
        load_key, no_load_key, temp_dict = [], [], {}

        # éå†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æ¯ä¸ªå‚æ•°
        for k, v in pretrained_dict.items():
            # å¦‚æœå½“å‰æ¨¡å‹ä¸­æœ‰å¯¹åº”çš„å‚æ•°ï¼Œå¹¶ä¸”å½¢çŠ¶åŒ¹é…ï¼Œåˆ™åŠ è½½è¯¥å‚æ•°
            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):
                temp_dict[k] = v  # å°†åŒ¹é…çš„å‚æ•°å­˜å‚¨åˆ°ä¸´æ—¶å­—å…¸ä¸­
                load_key.append(k)  # è®°å½•æˆåŠŸåŠ è½½çš„å‚æ•°çš„Key
            else:
                no_load_key.append(k)  # è®°å½•æ²¡æœ‰åŒ¹é…ä¸Šçš„å‚æ•°çš„Key

        # æ›´æ–°æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°åŠ è½½åˆ°å½“å‰æ¨¡å‹ä¸­
        model_dict.update(temp_dict)
        # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
        model.load_state_dict(model_dict, strict=False)
        
        # æ‰“å°åŠ è½½ä¿¡æ¯
        print(f"ğŸ“¦ æƒé‡åŠ è½½å®Œæˆ: æˆåŠŸåŠ è½½ {len(load_key)} ä¸ªå‚æ•°")
        if no_load_key:
            print(f"âš ï¸  æ— æ³•åŠ è½½ {len(no_load_key)} ä¸ªå‚æ•°ï¼ˆå¯èƒ½æ˜¯æ¶æ„å˜åŒ–å¯¼è‡´çš„ï¼‰")
    
    return model





def get_optimizer_and_lr(model, batch_size, train_epoch, momentum, weight_decay):
    # åˆå§‹åŒ–å­¦ä¹ ç‡ï¼ˆåˆå§‹å­¦ä¹ ç‡ä¸º1e-4ï¼‰
    Init_lr = 1e-4
    # æœ€å°å­¦ä¹ ç‡æ˜¯åˆå§‹å­¦ä¹ ç‡çš„1%
    Min_lr = Init_lr * 0.01

    # è®¾ç½®å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œ'cos'ä»£è¡¨ä½™å¼¦è¡°å‡
    lr_decay_type = 'cos'

    # é»˜è®¤æ¯ä¸ªå°æ‰¹é‡ï¼ˆbatchï¼‰çš„å¤§å°ä¸º16
    nbs = 16
    # è®¾ç½®æœ€å¤§å­¦ä¹ ç‡é™åˆ¶
    lr_limit_max = 1e-4
    # è®¾ç½®æœ€å°å­¦ä¹ ç‡é™åˆ¶
    lr_limit_min = 1e-4

    # æ ¹æ®å½“å‰batch_sizeè°ƒæ•´å­¦ä¹ ç‡ï¼Œå¹¶ç¡®ä¿å®ƒåœ¨æœ€å¤§å’Œæœ€å°é™åˆ¶ä¹‹é—´
    Init_lr_fit = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)
    # æ ¹æ®å½“å‰batch_sizeè°ƒæ•´æœ€å°å­¦ä¹ ç‡ï¼Œå¹¶ç¡®ä¿å®ƒåœ¨æœ€å¤§å’Œæœ€å°é™åˆ¶ä¹‹é—´
    Min_lr_fit = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)

    # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œè®¾ç½®å­¦ä¹ ç‡ã€åŠ¨é‡ã€æƒé‡è¡°å‡ç­‰å‚æ•°
    optimizer = optim.Adam(model.parameters(), Init_lr_fit, betas=(momentum, 0.999),
                           weight_decay=weight_decay)
    # è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨å‡½æ•°ï¼Œæ ¹æ®è¡°å‡ç±»å‹ã€åˆå§‹å­¦ä¹ ç‡ã€æœ€å°å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ¬¡æ¥è®¡ç®—å­¦ä¹ ç‡çš„å˜åŒ–
    lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, train_epoch)
    # è¿”å›ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    return optimizer, lr_scheduler_func


def train(args):
    seed_everything(11)  # è®¾ç½®ç§å­

    num_classes = args.num_classes + 1  # ç±»åˆ«åŠ ä¸ŠèƒŒæ™¯ç±»
    train_epoch = args.epochs  # è®­ç»ƒè½®æ¬¡
    batch_size = args.batch_size  # è®¾ç½®batch sizeå’Œç±»åˆ«æ•°
    num_workers = args.workers  # è®¡ç®—å¯ç”¨çš„å·¥ä½œçº¿ç¨‹æ•°ï¼Œé€šå¸¸å–CPUæ ¸å¿ƒæ•°ã€batch_sizeå’Œ8ä¸­çš„æœ€å°å€¼

    # é€‰æ‹©è®¾å¤‡ï¼ˆGPU å¦‚æœå¯ç”¨ï¼Œå¦åˆ™ä½¿ç”¨ CPUï¼‰
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    # è°ƒç”¨å‡½æ•°è·å–æ–°çš„expæ–‡ä»¶å¤¹å’Œweightsæ–‡ä»¶å¤¹è·¯å¾„
    exp_folder, weights_folder = create_exp_folder()

    input_shape = [1024, 1024]  # ä¸€å®šè¦æ˜¯32çš„æ•´æ•°å€

    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†å¯¹è±¡
    # args.data_path: æ•°æ®é›†çš„æ ¹è·¯å¾„ï¼Œ input_shape: è¾“å…¥å›¾åƒçš„å°ºï¼Œnum_classes: è¾“å‡ºç±»åˆ«æ•°ï¼Œè¡¨ç¤ºåˆ†å‰²ä»»åŠ¡ä¸­çš„ç±»åˆ«æ•°
    # augmentation=True: æ˜¯å¦é‡‡ç”¨æ•°æ®å¢å¼ºï¼Œtxt_name="train.txt": æŒ‡å®šç”¨äºåŠ è½½è®­ç»ƒæ•°æ®çš„æ–‡æœ¬æ–‡ä»¶å
    train_dataset = UnetDataset(args.data_path, input_shape, num_classes, augmentation=True, txt_name="train.txt")
    val_dataset = UnetDataset(args.data_path, input_shape, num_classes, augmentation=False, txt_name="val.txt")

    # åŠ è½½è®­ç»ƒé›†çš„DataLoader
    train_loader = DataLoader(train_dataset,
                              shuffle=True,  # æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆè®­ç»ƒæ—¶ä¸€èˆ¬æ‰“ä¹±æ•°æ®ï¼‰
                              batch_size=batch_size,  # æ¯ä¸ªæ‰¹æ¬¡åŠ è½½çš„æ ·æœ¬æ•°é‡
                              num_workers=num_workers,  # åŠ è½½æ•°æ®æ—¶ä½¿ç”¨çš„å­è¿›ç¨‹æ•°é‡ï¼ˆå¹¶è¡ŒåŒ–åŠ è½½ï¼‰
                              pin_memory=True,  # æ˜¯å¦å°†æ•°æ®å¤åˆ¶åˆ°CUDAçš„å†…å­˜ä¸­ï¼ˆå¦‚æœä½¿ç”¨GPUè®­ç»ƒï¼Œé€šå¸¸è®¾ç½®ä¸ºTrueï¼‰
                              drop_last=False,  # å¦‚æœæ•°æ®é›†å¤§å°ä¸èƒ½è¢«batch_sizeæ•´é™¤ï¼Œæ˜¯å¦ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
                              collate_fn=unet_dataset_collate,  # å®šä¹‰å¦‚ä½•å°†å¤šä¸ªæ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œé€šå¸¸æ˜¯å¤„ç†ä¸åŒå¤§å°çš„å›¾åƒ
                              sampler=None,  # æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰é‡‡æ ·å™¨ï¼Œé»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºæŒ‰é¡ºåºåŠ è½½
                              worker_init_fn=partial(worker_init_fn, rank=0, seed=11))  # åˆå§‹åŒ–workeræ—¶çš„å‡½æ•°ï¼Œé€šå¸¸ç”¨äºè®¾ç½®éšæœºç§å­

    # åŠ è½½éªŒè¯é›†çš„DataLoader
    val_loader = DataLoader(val_dataset,
                            shuffle=True,
                            batch_size=batch_size,
                            num_workers=num_workers,
                            pin_memory=True,
                            drop_last=False,
                            collate_fn=unet_dataset_collate,
                            sampler=None,
                            worker_init_fn=partial(worker_init_fn, rank=0, seed=11))

    # åˆ›å»ºæ¨¡å‹ï¼ˆæ”¯æŒå¢å¼ºç‰ˆå’ŒåŸç‰ˆï¼‰
    model = create_model(
        num_classes=num_classes, 
        weights=args.weights,
        use_enhanced=args.use_enhanced,
        use_attention=args.use_attention,
        use_transpose=args.use_transpose,
        use_nucleus_cytoplasm=args.use_nucleus_cytoplasm
    )
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡: æ€»å‚æ•°={total_params/1e6:.2f}M, å¯è®­ç»ƒå‚æ•°={trainable_params/1e6:.2f}M")

    # å¦‚æœå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆampï¼‰ï¼Œä½¿ç”¨GradScaler
    scaler = torch.cuda.amp.GradScaler() if args.amp else None
    # scaler = torch.amp.GradScaler(device='cuda') if args.amp else None  # åœ¨pytorch2.7.1ç‰ˆæœ¬å¯ä»¥è¿ç”¨è¯¥ä»£ç å¯ä»¥å»é™¤è­¦å‘Šä¿¡æ¯

    # è·å–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer, lr_scheduler_func = get_optimizer_and_lr(model, batch_size, train_epoch,  args.momentum, args.weight_decay)

    # è®­ç»ƒå¼€å§‹
    start_time = time.time()

    best_acc = 0.0  # æœ€ä¼˜å‡†ç¡®ç‡åˆå§‹åŒ–ä¸º0
    best_model_path = os.path.join(weights_folder, f"best_model_{args.num_classes}.pth")  # æœ€ä¼˜æ¨¡å‹ä¿å­˜è·¯å¾„
    last_model_path = os.path.join(weights_folder, f"last_model_{args.num_classes}.pth")  # æœ€åä¸€è½®æ¨¡å‹ä¿å­˜è·¯å¾„

    train_losses = []
    val_losses = []
    val_metrics_history = []

    # æ˜¯å¦ä½¿ç”¨focal lossæ¥é˜²æ­¢æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡ï¼Œæ˜¯å¦ç»™ä¸åŒç§ç±»èµ‹äºˆä¸åŒçš„æŸå¤±æƒå€¼ï¼Œé»˜è®¤æ˜¯å¹³è¡¡çš„ã€‚
    focal_loss = True
    #  ç§ç±»å°‘ï¼ˆå‡ ç±»ï¼‰æ—¶ï¼Œè®¾ç½®ä¸ºTrue
    #  ç§ç±»å¤šï¼ˆåå‡ ç±»ï¼‰æ—¶ï¼Œå¦‚æœbatch_sizeæ¯”è¾ƒå¤§ï¼ˆ10ä»¥ä¸Šï¼‰ï¼Œé‚£ä¹ˆè®¾ç½®ä¸ºTrue
    #  ç§ç±»å¤šï¼ˆåå‡ ç±»ï¼‰æ—¶ï¼Œå¦‚æœbatch_sizeæ¯”è¾ƒå°ï¼ˆ10ä»¥ä¸‹ï¼‰ï¼Œé‚£ä¹ˆè®¾ç½®ä¸ºFalse
    dice_loss = True

    #   å¼€å§‹æ¨¡å‹è®­ç»ƒ
    for epoch in range(train_epoch):
        gpu_used = get_gpu_usage() # è®¡ç®—ä½¿ç”¨GPUå†…å­˜
        set_optimizer_lr(optimizer, lr_scheduler_func, epoch)  # å­¦ä¹ ç‡è°ƒåº¦å‡½æ•°

        # æ¯ä¸ªepochè¿›è¡Œè®­ç»ƒ
        loss = train_one_epoch(model, optimizer, train_loader, device, dice_loss, focal_loss,
                               gpu_used, num_classes, scaler, epoch, train_epoch)

        train_losses.append(loss)  # ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„losså€¼

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
        metrics = evaluate(model, val_loader, device, dice_loss, focal_loss, num_classes)

        val_losses.append(metrics["Loss"])
        val_metrics_history.append(metrics)

        current_acc = float(metrics["Mean Accuracy"])  # è½¬æ¢ä¸ºæµ®åŠ¨å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰

        # æ›´æ–°æœ€ä¼˜å‡†ç¡®ç‡å¹¶ä¿å­˜æœ€ä¼˜æ¨¡å‹
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if current_acc > best_acc:
            best_acc = current_acc
            torch.save(model.state_dict(), best_model_path)

        # ä¿å­˜æœ€åä¸€æ¬¡æ¨¡å‹
        torch.save(model.state_dict(), last_model_path)

    # æ‰“å°è®­ç»ƒçš„æ€»æ—¶é•¿
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print("training time {}".format(total_time_str))

    # æœ€åç”»å›¾
    plot_training_curves(train_losses, val_losses, val_metrics_history, weights_folder)


def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description="pytorch fcn training")
    parser.add_argument("--weights", default=None,
                        help="Path to the directory containing model weights")
    parser.add_argument("--data-path", default=r"E:\Data_Industry\split_images\edit_1004560_MergedBatch", help="VOCdevkit root")
    parser.add_argument("--num-classes", default=1, type=int)
    parser.add_argument("--device", default="cuda", help="training device")
    parser.add_argument("--batch-size", default=2, type=int)
    parser.add_argument("--epochs", default=30, type=int, metavar="N", help="number of total epochs to train")
    parser.add_argument("--workers", default=0, type=int, metavar="N",
                        help="number of data loading workers (default: 0, meaning data loading runs in main process)")
    parser.add_argument('--lr', default=0.000001, type=float, help='initial learning rate')
    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')
    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,
                        metavar='W', help='weight decay (default: 1e-4)',
                        dest='weight_decay')
    # Mixed precision training parameters
    parser.add_argument("--amp", default=False, type=bool, help="Use torch.cuda.amp for mixed precision training")
    
    # å¢å¼ºç‰ˆUNetå‚æ•°
    parser.add_argument("--use-enhanced", default=True, type=bool, 
                       help="æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆUNetï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨æ³¨æ„åŠ›é—¨æ§å’Œè½¬ç½®å·ç§¯ï¼‰")
    parser.add_argument("--use-attention", default=True, type=bool,
                       help="æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›é—¨æ§ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰")
    parser.add_argument("--use-transpose", default=True, type=bool,
                       help="æ˜¯å¦ä½¿ç”¨è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰")
    parser.add_argument("--use-nucleus-cytoplasm", default=True, type=bool,
                       help="æ˜¯å¦ä½¿ç”¨æ ¸è´¨åˆ†ç¦»æ„ŸçŸ¥æ¨¡å—ï¼ˆä»…å¢å¼ºç‰ˆæœ‰æ•ˆï¼Œé»˜è®¤Trueï¼‰")
    
    args = parser.parse_args()

    return args


if __name__ == "__main__":
    args = parse_args()
    train(args)